{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting disaster tweets using LSTM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1R_hLVLGAf4mvUAPHGwcJzjNh2vBnQ8XN",
      "authorship_tag": "ABX9TyNgeLqNymsaK7HbhbBswdwR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedOssamaAhmed/disaster-tweets-LSTM/blob/main/Predicting_disaster_tweets_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- Load the dataset"
      ],
      "metadata": {
        "id": "CTZkeLQTKQZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ombEP7TyKUjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk,csv,numpy \n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "reader = csv.reader(open('/content/drive/MyDrive/predictiong disaster tweet/train.csv', 'r'), delimiter= \",\")\n",
        "\n",
        "\n",
        "\n",
        "tweets = []\n",
        "target = []\n",
        "location =[]\n",
        "keyword = []\n",
        "for i in reader:\n",
        "  tweets.append(i[3])\n",
        "  target.append(i[4])\n",
        "  keyword.append(i[1])\n",
        "  location.append(i[2])\n",
        "\n",
        "\n",
        "print(f\" tweets count : {len(tweets)}\")\n",
        "print(f\" keyword count : {len(keyword)}\")\n",
        "print(f\" location count : {len(location)}\")\n",
        "print(f\" target count : {len(target)}\")\n"
      ],
      "metadata": {
        "id": "kGz_R2jbLWSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Check head and info of the data"
      ],
      "metadata": {
        "id": "XhVm8s1FvYZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(keyword)\n",
        "print(location)\n",
        "print(tweets)\n",
        "print(target)\n",
        "keyword.pop(0)\n",
        "location.pop(0)\n",
        "tweets.pop(0)\n",
        "target.pop(0)\n",
        "print(keyword)\n",
        "print(location)\n",
        "print(tweets)\n",
        "print(target)"
      ],
      "metadata": {
        "id": "LQ8pxVwFvZEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3- Is there a missing data [how many and the percentage if there]?"
      ],
      "metadata": {
        "id": "Ewhy-OcWW2wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_percentage(data):\n",
        "  total_data = len(data)\n",
        "  for j in range(10):\n",
        "    for i in data:\n",
        "      if i == \"\":\n",
        "        data.remove(i)\n",
        "  data_no_missing = len(data)\n",
        "  return ((7614 - data_no_missing)/7614)*100\n",
        "\n"
      ],
      "metadata": {
        "id": "3YaE54o-XDEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the percentage of missing keywords is {missing_percentage(keyword)}\")\n",
        "print(f\"the percentage of missing location is {missing_percentage(location)}\")\n",
        "print(f\"the percentage of missing tweets is {missing_percentage(tweets)}\")\n",
        "print(f\"the percentage of missing target is {missing_percentage(target)}\")"
      ],
      "metadata": {
        "id": "F-8EMXIgXAHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the percentage of missing keywords is 0.8142894667717363 %\n",
        "\n",
        "the percentage of missing location is 33.28079852902548 %\n",
        "\n",
        "the percentage of missing tweets is 0.013133701076963489 %\n",
        "\n",
        "the percentage of missing target is 0.013133701076963489 %\n"
      ],
      "metadata": {
        "id": "UyVFogIOqegM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4- How many data in each class?"
      ],
      "metadata": {
        "id": "JhyxithWqtAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(target)\n",
        "real_disaster_counter = 0\n",
        "not_disaster = 0\n",
        "for i in target:\n",
        "  if i == '1':\n",
        "    real_disaster_counter +=1\n",
        "  elif i == '0':\n",
        "    not_disaster +=1\n",
        "print(f\"the count of real disaster class is {real_disaster_counter}\")\n",
        "print(f\"the count of not disaster class is {not_disaster}\")"
      ],
      "metadata": {
        "id": "6oblNxXKqtnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5- Get the top 15 locations of the data"
      ],
      "metadata": {
        "id": "rERWJDTLsxx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_15(data):\n",
        "  freq_dict = {}\n",
        "  for i in data:\n",
        "    if i not in freq_dict:\n",
        "      freq_dict[i] = 1\n",
        "    else:\n",
        "      freq_dict[i] +=1\n",
        "\n",
        "  sorted_dict = dict(sorted(freq_dict.items(), key=lambda item: item[1], reverse = True))\n",
        "  top_15_items = []\n",
        "  for i in range(15):\n",
        "    top_15_items.append(list(sorted_dict.keys())[i])\n",
        "\n",
        "  return top_15_items"
      ],
      "metadata": {
        "id": "O1FGLjEGsz5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_locations = top_15(location)\n",
        "print(top_15_locations)\n"
      ],
      "metadata": {
        "id": "xkv-CsBovAGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6- Get the top 15 keyword in the data"
      ],
      "metadata": {
        "id": "W32femFh6N0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_keywords = top_15(keyword)\n",
        "print(top_15_keywords)"
      ],
      "metadata": {
        "id": "fOtNChva6OdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7- What are the most common words?\n",
        "\n",
        "  8- What are the most common stop words?"
      ],
      "metadata": {
        "id": "uGvW9OET6ob0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "6rQ8EKdt6pIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "print(string.punctuation)\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "7HkV7OL8u5t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def cleanText(text):\n",
        "    text=re.sub(r'#[a-zA-Z0-9|_]*','',text)#  remove hashtage \n",
        "    text=re.sub(r'@[a-zA-Z0-9|_]*','',text)# remove user name \n",
        "    text=re.sub(r'https?:\\/\\/\\S+','',text)#remove hyperlink \n",
        "    text=re.sub(r'\\W',' ',text) # remove emotions\n",
        "    text=re.sub('\\d+','',text) # remove digits\n",
        "    text=re.sub(r'^\\s+','',text)# remove space in front of text \n",
        "    text=re.sub(r'\\s+$','',text) #remove space in tail text\n",
        "    return text"
      ],
      "metadata": {
        "id": "vo8atHfDtqfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_copy = tweets\n",
        "result = []\n",
        "for i in tweets_copy:\n",
        "  tweet=cleanText(i)\n",
        "  tweet=tweet.split()\n",
        "  result.append(tweet)\n"
      ],
      "metadata": {
        "id": "X0GCEh7_Ckq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dict={}\n",
        "for i in range(len(result)):\n",
        "  for j in range(len(result[i])):\n",
        "    if result[i][j] not in common_dict:\n",
        "      common_dict[result[i][j]] = 1\n",
        "    else:common_dict[result[i][j]] += 1\n",
        "\n",
        "sorted_dict = dict(sorted(common_dict.items(), key=lambda item: item[1], reverse = True))\n",
        "top_5_items = []\n",
        "for i in range(5):\n",
        "  top_5_items.append(list(sorted_dict.keys())[i])\n",
        "print(top_5_items)"
      ],
      "metadata": {
        "id": "PCHfr81KD_sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the most common stop words are :\n",
        "['the', 'a', 'to', 'in', 'of']"
      ],
      "metadata": {
        "id": "uqzB5d-PFiwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    result=[]\n",
        "    for tweet in tweets:\n",
        "\n",
        "        tweet=cleanText(tweet)\n",
        "        tweet=tweet.split()\n",
        "        #remove stop words \n",
        "        tweet=[word for word in tweet  if word.lower() not in stop_words]\n",
        "        #stemming \n",
        "        ps=PorterStemmer()\n",
        "\n",
        "        tweet=[ps.stem(word) for word in tweet ]\n",
        "\n",
        "        result.append(tweet)\n",
        "    return result "
      ],
      "metadata": {
        "id": "9N1AoVAprnHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_processed = process_tweets(tweets)\n",
        "print(tweets_processed)\n"
      ],
      "metadata": {
        "id": "AR0jEd1NrFxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_words = {}\n",
        "for i in range(len(tweets_processed)):\n",
        "  for j in range(len(tweets_processed[i])):\n",
        "    if tweets_processed[i][j] not in common_words:\n",
        "      common_words[tweets_processed[i][j]] = 1\n",
        "    else:common_words[tweets_processed[i][j]] +=1\n",
        "sorted_dict = dict(sorted(common_words.items(), key=lambda item: item[1], reverse = True))\n",
        "top_5_items = []\n",
        "for i in range(5):\n",
        "  top_5_items.append(list(sorted_dict.keys())[i])\n",
        "print(top_5_items)"
      ],
      "metadata": {
        "id": "J7kVvG9Nq1yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the most common words are :['like', 'fire', 'û_', 'amp', 'get']\n",
        "\n"
      ],
      "metadata": {
        "id": "8lSStM5gHPFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9- Use nlp to prepare dataset [tokenization, pad sequence, etc.]"
      ],
      "metadata": {
        "id": "Ieh1akj-IR9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_processed)"
      ],
      "metadata": {
        "id": "fz2QoF53qx1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding"
      ],
      "metadata": {
        "id": "dkHaCc6_Lfx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max = 0\n",
        "min = 900\n",
        "for i in tweets_processed:\n",
        "  if len(i) > max:\n",
        "    max = len(i)\n",
        "  if len(i) < min:\n",
        "    min = len(i)\n",
        "print(max,min)"
      ],
      "metadata": {
        "id": "Ei7kVV4MPf4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_words = []\n",
        "for i in tweets_processed:\n",
        "  len_words.append(len(i))\n",
        "sum = 0\n",
        "for j in len_words:\n",
        "  sum = sum + j\n",
        "avg_len = int(sum/len(len_words))\n",
        "\n",
        "avg_len"
      ],
      "metadata": {
        "id": "s_8xlEdVLK8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for tweet in tweets_processed:\n",
        "#   if len(tweet) > avg_len:\n",
        "#     for k in range(len(tweet)-avg_len):\n",
        "#       tweet.pop(0)\n",
        "#   elif len(tweet) < avg_len:\n",
        "#     for j in range(avg_len - len(tweet)):\n",
        "#       tweet.append(\"o\")\n"
      ],
      "metadata": {
        "id": "CB2X3iauNt_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max = 0\n",
        "min = 900\n",
        "for i in tweets_processed:\n",
        "  if len(i) > max:\n",
        "    max = len(i)\n",
        "  if len(i) < min:\n",
        "    min = len(i)\n",
        "print(max,min)"
      ],
      "metadata": {
        "id": "ZuRhVuupPC-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_processed)\n",
        "tweets_lst = []\n",
        "for tweet in tweets_processed:\n",
        "    lst = \"\"\n",
        "    for word in tweet:\n",
        "        lst += word\n",
        "        lst += \" \"\n",
        "    tweets_lst.append(lst)\n",
        "print(tweets_lst)"
      ],
      "metadata": {
        "id": "OJPw-s0maM0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tweets_df = pd.DataFrame(tweets_lst,columns = [\"tweet\"])\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "i25-pgKaZmoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df[\"sentiment\"] = target\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "HmlOLiXjb-eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_df)"
      ],
      "metadata": {
        "id": "7fJtJ0PcdOh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10- Prepare train, test sets"
      ],
      "metadata": {
        "id": "zNr77KgDKXdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweets_df))\n",
        "print(tweets_df.shape)"
      ],
      "metadata": {
        "id": "-Svxz-dSqqIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(tweets_df,test_size=0.2,random_state=0,shuffle=True)\n"
      ],
      "metadata": {
        "id": "ggEp_ndpqlf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "id": "pFMpeL2If8jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_sentiment = {\"0\":\"Negative\", \"1\":\"Positive\"}\n",
        "def mapper(label):\n",
        "     return label_to_sentiment[label]\n",
        "X_train.sentiment = X_train.sentiment.apply(lambda x: mapper(x))\n",
        "X_test.sentiment = X_test.sentiment.apply(lambda x: mapper(x))\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "wadTze0JfrTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n"
      ],
      "metadata": {
        "id": "8QlpmwlWTGIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "Y9jNJfTKX-Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(X_train.tweet)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "ITBVc1LGeCAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "metadata": {
        "id": "7F5VSVHZeGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vdp4xgMQfhwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# The tokens are converted into sequences and then passed to the pad_sequences() function\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(X_train.tweet),maxlen = 8)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(X_test.tweet),maxlen = 8)"
      ],
      "metadata": {
        "id": "MjVV_QRieOPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Positive']\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(X_train.sentiment.to_list())\n",
        "y_train = encoder.transform(X_train.sentiment.to_list())\n",
        "y_test = encoder.transform(X_test.sentiment.to_list())\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "metadata": {
        "id": "mCA3iFXhebnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "AHPPl4yxgv-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11- Train your LSTM structure"
      ],
      "metadata": {
        "id": "bPTxDQrwUJoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "rVzCAoccHFxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "y2w1Pg-wHOAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "# opening the downloaded glove embeddings file\n",
        "f = open('glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    # For each line file, the words are split and stored in a list\n",
        "    values = line.split()\n",
        "    word = value = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ],
      "metadata": {
        "id": "ThlRhgpe_REl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an matrix with zeroes of shape vocab x embedding dimension\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "# Iterate through word, index in the dictionary\n",
        "for word, i in word_index.items():\n",
        "    # extract the corresponding vector for the vocab indice of same word\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Storing it in a matrix\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "BEIb0IIWIUXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "embedding_layer = tf.keras.layers.Embedding(vocab_size,300,weights=[embedding_matrix],\n",
        "                                          input_length=8,trainable=False)"
      ],
      "metadata": {
        "id": "ID02yxbsIU0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import various layers needed for the architecture from keras\n",
        "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# The Input layer \n",
        "sequence_input = Input(shape=(8,), dtype='int32')\n",
        "# Inputs passed to the embedding layer\n",
        "embedding_sequences = embedding_layer(sequence_input)\n",
        "# dropout and conv layer \n",
        "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "x = Conv1D(64, 5, activation='relu')(x)\n",
        "# Passed on to the LSTM layer\n",
        "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "# Passed on to activation layer to get final output\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(sequence_input, outputs)"
      ],
      "metadata": {
        "id": "Hb7FLFmp_R04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "LR =3e-4\n",
        "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',metrics=['accuracy'])\n",
        "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,min_lr = 0.01, monitor = 'val_loss',verbose = 1)"
      ],
      "metadata": {
        "id": "k1_BpwV-GkJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,\n",
        "                                                  restore_best_weights=True,\n",
        "                                                 monitor=\"val_accuracy\")\n",
        "training = model.fit(x_train, y_train, batch_size=256, epochs=50,\n",
        "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau,early_stopping_cb])"
      ],
      "metadata": {
        "id": "uWkAFItZJBqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12- Evaluate the model and make predictions"
      ],
      "metadata": {
        "id": "jJfFLsgwLFY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet_sentiment(score):\n",
        "    return \"Positive\" if score>0.5 else \"Negative\"\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "model_predictions = [predict_tweet_sentiment(score) for score in scores]\n",
        "print(model_predictions)"
      ],
      "metadata": {
        "id": "BUTnEBkeGdpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(list(X_test.sentiment), model_predictions))"
      ],
      "metadata": {
        "id": "ku3HmoktLMrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13- Evaluate the results with charts of acc and loss"
      ],
      "metadata": {
        "id": "2jJtu5TqMOuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(training.history['accuracy'])\n",
        "plt.plot(training.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yHl_oDrALaHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(training.history['loss'])\n",
        "plt.plot(training.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8P0crNGmM8RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14- Save your model"
      ],
      "metadata": {
        "id": "WkEWVbx9NDpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('saved_model/my_model')"
      ],
      "metadata": {
        "id": "woYVaiq1NGb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test data"
      ],
      "metadata": {
        "id": "oO3NKV8pPi4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader_test_final = csv.reader(open('/content/drive/MyDrive/predictiong disaster tweet/test.csv', 'r'), delimiter= \",\")\n",
        "tweets_test=[]\n",
        "for j in reader_test_final:\n",
        "    tweets_test.append(j[3])\n",
        "print(tweets_test)\n",
        "tweets_test.pop(0)\n",
        "tweets_copy = tweets_test\n",
        "result = []\n",
        "tweets_processed = process_tweets(tweets)\n",
        "print(tweets_processed)\n",
        "tweets_lst = []\n",
        "for tweet in tweets_processed:\n",
        "    lst = \"\"\n",
        "    for word in tweet:\n",
        "        lst += word\n",
        "        lst += \" \"\n",
        "    tweets_lst.append(lst)\n",
        "print(tweets_lst)\n",
        "import pandas as pd\n",
        "tweets_df = pd.DataFrame(tweets_lst,columns = [\"tweet\"])\n",
        "tweets_df.head()\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tweets_df.tweet)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)\n",
        "test = pad_sequences(tokenizer.texts_to_sequences(tweets_df.tweet),maxlen = 8)\n",
        "\n",
        "scores = model.predict(test, verbose=1, batch_size=10000)\n",
        "model_predictions_final = [predict_tweet_sentiment(score) for score in scores]\n",
        "print(model_predictions_final)"
      ],
      "metadata": {
        "id": "bMVrhDoNPj_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}